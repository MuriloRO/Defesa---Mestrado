{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d723daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodando GraphMinCutGrowing\n",
      "Testando usuário 50/50\n",
      "Genuino s0132\n",
      "00:03:10.67\n",
      "---------------------\n",
      "Teste com Grafos distancia Euclidiana\n",
      "GRAPH_MIN_CUT_GROWING [0.6509590199999999]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from biometric_system_euclidiano import BiometricSystem\n",
    "\n",
    "from anomaly_detectors.M2005 import M2005 \n",
    "from anomaly_detectors import thresholds\n",
    "from data_stream import data_stream\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import json\n",
    "import ipdb\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import copy\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "def Average(lst):\n",
    "    x = sum(lst)\n",
    "    y = len(lst)\n",
    "    a = (round(x,6) / round(y))\n",
    "    return a\n",
    "\n",
    "\n",
    "def split_data_enrollment(dataset, column, n_samples):\n",
    "    data_to_enrollment = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    "        \n",
    "        data_to_enrollment.setdefault(value, dataset.loc[dataset[column]==value].iloc[:(n_samples//2)].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    return data_to_enrollment\n",
    "\n",
    "def split_data_validation(dataset, column, n_samples):\n",
    "    data_to_validation = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    "        \n",
    "        data_to_validation.setdefault(value, dataset.loc[dataset[column]==value].iloc[(n_samples//2):n_samples].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    return data_to_validation\n",
    "\n",
    "\n",
    "def split_data_recognition(dataset, column, n_samples):\n",
    "    data_to_recognition = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    " \n",
    "        data_to_recognition.setdefault(value, dataset.loc[dataset[column]==value].iloc[:n_samples].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    \n",
    "    return data_to_recognition\n",
    "\n",
    "def split_data(dataset, column, n_samples):\n",
    "    data_to_enrollment = dict()\n",
    "    data_to_validation = dict()\n",
    "    data_to_recognition = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    "        \n",
    "        #Treinamento, pegando dados de usuarios \n",
    "        data_to_enrollment.setdefault(value, dataset.loc[dataset[column]==value].iloc[:(n_samples//2)].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "\n",
    "        # Definir o limiar de decisao dos algoritmos de classificacao\n",
    "        data_to_validation.setdefault(value, dataset.loc[dataset[column]==value].iloc[(n_samples//2):n_samples].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "\n",
    "        # Criar o fluxo de teste\n",
    "        data_to_recognition.setdefault(value, dataset.loc[dataset[column]==value].iloc[n_samples:].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    \n",
    "    return data_to_enrollment,data_to_validation, data_to_recognition\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "# INICIO DO TIMER\n",
    "inicio = timeit.default_timer()\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "# Supondo que você tenha um DataFrame chamado 'dados' com uma coluna 'subject' para os usuários\n",
    "dados = pd.read_excel('dados/Greyc.xlsx')\n",
    "\n",
    "# DataFrame para armazenar as 60 primeiras amostras de cada usuário que tem pelo menos 60 amostras\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Lista para armazenar os usuários que têm pelo menos 60 amostras e foram utilizados para criar o DataFrame\n",
    "users = []\n",
    "\n",
    "for usuario in dados['subject'].unique():\n",
    "    # Verificar se o usuário tem pelo menos 60 amostras\n",
    "    if dados[dados['subject'] == usuario].shape[0] >= 60:\n",
    "        # Selecionar as 60 primeiras amostras do usuário atual\n",
    "        amostras_usuario = dados[dados['subject'] == usuario].head(60)\n",
    "        \n",
    "        # Adicionar as amostras ao DataFrame principal\n",
    "        df = pd.concat([df, amostras_usuario])\n",
    "        \n",
    "        # Adicionar o usuário à lista de usuários utilizados\n",
    "        users.append(usuario)\n",
    "\n",
    "\n",
    "perc = 0.5\n",
    "impostor_rate = 0.30\n",
    "rate_external_impostor = 0\n",
    "R=10\n",
    "GRAPH_MIN_CUT_GROWING = []\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "#Separação de index, primeiro pra treino, segundo para validação e teste\n",
    "# Dependendo do teste é necessario trocar a linha (\"Separação por Index\")\n",
    "\n",
    "sessionIndex1 = 1\n",
    "sessionIndex2 = 2\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "#Separação de usuarios\n",
    "\n",
    "len_reg_users = int(len(users) * perc)\n",
    "\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=R)\n",
    "splits = kfold.split(users)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "#Registro de usuarios\n",
    "users_array = np.array(users)\n",
    "\n",
    "for i, (reg_users, not_reg_users) in enumerate(splits):\n",
    "    # Convertendo os índices reg_users para um numpy array\n",
    "    reg_users_array = np.array(reg_users)\n",
    "    \n",
    "    # Usando os índices reg_users para indexar o array de usuários\n",
    "    internal_users = copy.deepcopy(df.loc[df['subject'].isin(users_array[reg_users_array])])\n",
    "    external_users = copy.deepcopy(df.loc[~df['subject'].isin(users_array[reg_users_array])])\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------#  \n",
    "#Separação por Index\n",
    "\n",
    "dataS1 = internal_users.loc[(internal_users['sessionIndex'] == sessionIndex1)]\n",
    "dataS1.drop([\"sessionIndex\"], axis=1, inplace=True)\n",
    "\n",
    "dataS2 = internal_users.loc[(internal_users['sessionIndex'] != sessionIndex1)]\n",
    "dataS2.drop([\"sessionIndex\"], axis=1, inplace=True)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#  \n",
    "# Dados para Treino, Validação e Reconhecimento\n",
    "\n",
    "#Treinamento, pegando dados de usuarios \n",
    "data_to_enrollment = split_data_enrollment(dataS1, column='subject', n_samples=10) #sempre pega metade, a outra metade\n",
    "#vai pra validacao\n",
    "\n",
    "# Definir o limiar de decisao dos algoritmos de classificacao\n",
    "data_to_validation = split_data_validation(dataS1, column='subject', n_samples=10)\n",
    "\n",
    "# Criar o fluxo de teste\n",
    "data_to_recognition = split_data_recognition(dataS2, column='subject', n_samples=50)\n",
    "\n",
    "_, _, external_users_data = split_data(external_users, column='subject', n_samples=50)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "\n",
    "#Sistema com Adaptação (GraphMinCut)\n",
    "\n",
    "metrics_adaptativo_grafo_growing = dict()\n",
    "lista_nao_usadas_grafo_growing = list()\n",
    "lista_usadas_grafo_growing = list()\n",
    "\n",
    "detector = M2005()\n",
    "adaptive= \"GrowingWindow\"\n",
    "system = BiometricSystem(detector=detector, random_state=R)\n",
    "system.enrollment_grafos(dataset=data_to_enrollment, adaptive=adaptive)\n",
    "\n",
    "decision_threshold = thresholds.best_threshold(data_to_validation, system, size=10, random_state=R)\n",
    "\n",
    "auxiliar_euclidiano = {}\n",
    "\n",
    "for j, genuine in enumerate(system.users.keys()):\n",
    "\n",
    "\n",
    "    ipd.clear_output(wait=True)\n",
    "    print(f\"Rodando GraphMinCutGrowing\")\n",
    "    print(f\"Testando usuário {j+1}/{len(system.users.keys())}\")\n",
    "    print(f\"Genuino\", genuine)\n",
    "\n",
    "    datastream = data_stream.Random(impostor_rate= impostor_rate,\n",
    "                                    rate_external_impostor=rate_external_impostor,\n",
    "                                    random_state=R)\n",
    "\n",
    "    test_stream, y_true, amostras_grafo_growing_genuinas,amostras_grafo_growing_impostoras = datastream.create(genuine,\n",
    "                                                                                                data_to_recognition,\n",
    "                                                                                                external_users_data)\n",
    "\n",
    "\n",
    "\n",
    "    y_pred, lista_nao_usadas_grafo_growing2, lista_usadas_grafo_growing2 = system.autenticate_grafos(genuine,\n",
    "                                                                                test_stream,\n",
    "                                                                                decision_threshold=decision_threshold,\n",
    "                                                                                adaptive_TESTE=adaptive,\n",
    "                                                                                return_scores=False)\n",
    "\n",
    "    lista_nao_usadas_grafo_growing.append(lista_nao_usadas_grafo_growing2)\n",
    "    lista_usadas_grafo_growing.append(lista_usadas_grafo_growing2)\n",
    "\n",
    "    fmr, fnmr, b_acc,  y_genuine , y_impostor = system.compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    count_1_genuine = y_genuine.value_counts().get(1, 0)\n",
    "    count_0_impostor = y_impostor.value_counts().get(0, 0)\n",
    "\n",
    "    auxiliar_euclidiano[genuine] = (count_1_genuine, count_0_impostor)\n",
    "\n",
    "    for met in ['fmr','fnmr','b_acc']:\n",
    "        metrics_adaptativo_grafo_growing.setdefault(genuine, dict()).setdefault(met,[]).append(eval(met))\n",
    "    #json.dump(metrics_adaptativo_grafo_growing, open(\"metricas_grafos_growing.json\", \"w\"))\n",
    "\n",
    "\n",
    "usuarios = metrics_adaptativo_grafo_growing.keys()\n",
    "result = pd.DataFrame(metrics_adaptativo_grafo_growing.values())\n",
    "\n",
    "fmr_mean = []\n",
    "fnmr_mean = []\n",
    "b_acc_mean = []\n",
    "\n",
    "for i in result['fmr']:\n",
    "    fmr_mean.append(Average(i))\n",
    "\n",
    "for i in result['fnmr']:\n",
    "    fnmr_mean.append(Average(i))\n",
    "\n",
    "for i in result['b_acc']:\n",
    "    b_acc_mean.append(Average(i))\n",
    "\n",
    "metrics_adaptativo_grafo_growing_mean = pd.DataFrame(list(zip(usuarios, fmr_mean, fnmr_mean,b_acc_mean)),\n",
    "            columns =['Usuarios','fmr_mean', 'fnmr_mean','b_acc_mean'])\n",
    "\n",
    "GRAPH_MIN_CUT_GROWING.append(metrics_adaptativo_grafo_growing_mean['b_acc_mean'].mean())\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#     \n",
    "\n",
    "fim = timeit.default_timer()\n",
    "horas, rem = divmod(fim-inicio, 3600)\n",
    "minutos, segundos = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(horas),int(minutos),segundos))\n",
    "\n",
    "print('---------------------')\n",
    "print(\"Teste com Grafos distancia Euclidiana\")\n",
    "print(\"GRAPH_MIN_CUT_GROWING\", GRAPH_MIN_CUT_GROWING)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0828c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodando GraphMinCutSliding Euclidiano\n",
      "Testando usuário 50/50\n",
      "00:03:03.42\n",
      "---------------------\n",
      "Teste com Grafos distancia Euclidiana\n",
      "GRAPH_MIN_CUT_SLIDING [0.7848070199999998]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from biometric_system_euclidiano import BiometricSystem\n",
    "\n",
    "from anomaly_detectors.M2005 import M2005 \n",
    "from anomaly_detectors import thresholds\n",
    "from data_stream import data_stream\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import json\n",
    "import ipdb\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import copy\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "def Average(lst):\n",
    "    x = sum(lst)\n",
    "    y = len(lst)\n",
    "    a = (round(x,6) / round(y))\n",
    "    return a\n",
    "\n",
    "\n",
    "def split_data_enrollment(dataset, column, n_samples):\n",
    "    data_to_enrollment = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    "        \n",
    "        data_to_enrollment.setdefault(value, dataset.loc[dataset[column]==value].iloc[:(n_samples//2)].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    return data_to_enrollment\n",
    "\n",
    "def split_data_validation(dataset, column, n_samples):\n",
    "    data_to_validation = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    "        \n",
    "        data_to_validation.setdefault(value, dataset.loc[dataset[column]==value].iloc[(n_samples//2):n_samples].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    return data_to_validation\n",
    "\n",
    "\n",
    "def split_data_recognition(dataset, column, n_samples):\n",
    "    data_to_recognition = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    " \n",
    "        data_to_recognition.setdefault(value, dataset.loc[dataset[column]==value].iloc[:n_samples].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    \n",
    "    return data_to_recognition\n",
    "\n",
    "def split_data(dataset, column, n_samples):\n",
    "    data_to_enrollment = dict()\n",
    "    data_to_validation = dict()\n",
    "    data_to_recognition = dict()\n",
    "\n",
    "    for value in dataset[column].unique():\n",
    "        \n",
    "        #Treinamento, pegando dados de usuarios \n",
    "        data_to_enrollment.setdefault(value, dataset.loc[dataset[column]==value].iloc[:(n_samples//2)].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "\n",
    "        # Definir o limiar de decisao dos algoritmos de classificacao\n",
    "        data_to_validation.setdefault(value, dataset.loc[dataset[column]==value].iloc[(n_samples//2):n_samples].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "\n",
    "        # Criar o fluxo de teste\n",
    "        data_to_recognition.setdefault(value, dataset.loc[dataset[column]==value].iloc[n_samples:].loc[:,~dataset.columns.isin([column])].reset_index(drop=True))\n",
    "    \n",
    "    \n",
    "    return data_to_enrollment,data_to_validation, data_to_recognition\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "# INICIO DO TIMER\n",
    "inicio = timeit.default_timer()\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "# Supondo que você tenha um DataFrame chamado 'dados' com uma coluna 'subject' para os usuários\n",
    "dados = pd.read_excel('dados/Greyc.xlsx')\n",
    "\n",
    "# DataFrame para armazenar as 60 primeiras amostras de cada usuário que tem pelo menos 60 amostras\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Lista para armazenar os usuários que têm pelo menos 60 amostras e foram utilizados para criar o DataFrame\n",
    "users = []\n",
    "\n",
    "for usuario in dados['subject'].unique():\n",
    "    # Verificar se o usuário tem pelo menos 60 amostras\n",
    "    if dados[dados['subject'] == usuario].shape[0] >= 60:\n",
    "        # Selecionar as 60 primeiras amostras do usuário atual\n",
    "        amostras_usuario = dados[dados['subject'] == usuario].head(60)\n",
    "        \n",
    "        # Adicionar as amostras ao DataFrame principal\n",
    "        df = pd.concat([df, amostras_usuario])\n",
    "        \n",
    "        # Adicionar o usuário à lista de usuários utilizados\n",
    "        users.append(usuario)\n",
    "\n",
    "\n",
    "perc = 0.5\n",
    "impostor_rate = 0.30\n",
    "rate_external_impostor = 0\n",
    "R=10\n",
    "GRAPH_MIN_CUT_SLIDING = []\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "#Separação de index, primeiro pra treino, segundo para validação e teste\n",
    "# Dependendo do teste é necessario trocar a linha (\"Separação por Index\")\n",
    "\n",
    "sessionIndex1 = 1\n",
    "sessionIndex2 = 2\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "#Separação de usuarios\n",
    "\n",
    "len_reg_users = int(len(users) * perc)\n",
    "\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=R)\n",
    "splits = kfold.split(users)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------# \n",
    "#Registro de usuarios\n",
    "users_array = np.array(users)\n",
    "\n",
    "for i, (reg_users, not_reg_users) in enumerate(splits):\n",
    "    # Convertendo os índices reg_users para um numpy array\n",
    "    reg_users_array = np.array(reg_users)\n",
    "    \n",
    "    # Usando os índices reg_users para indexar o array de usuários\n",
    "    internal_users = copy.deepcopy(df.loc[df['subject'].isin(users_array[reg_users_array])])\n",
    "    external_users = copy.deepcopy(df.loc[~df['subject'].isin(users_array[reg_users_array])])\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------#  \n",
    "#Separação por Index\n",
    "\n",
    "dataS1 = internal_users.loc[(internal_users['sessionIndex'] == sessionIndex1)]\n",
    "dataS1.drop([\"sessionIndex\"], axis=1, inplace=True)\n",
    "\n",
    "dataS2 = internal_users.loc[(internal_users['sessionIndex'] != sessionIndex1)]\n",
    "dataS2.drop([\"sessionIndex\"], axis=1, inplace=True)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#  \n",
    "# Dados para Treino, Validação e Reconhecimento\n",
    "\n",
    "#Treinamento, pegando dados de usuarios \n",
    "data_to_enrollment = split_data_enrollment(dataS1, column='subject', n_samples=10) #sempre pega metade, a outra metade\n",
    "#vai pra validacao\n",
    "\n",
    "# Definir o limiar de decisao dos algoritmos de classificacao\n",
    "data_to_validation = split_data_validation(dataS1, column='subject', n_samples=10)\n",
    "\n",
    "# Criar o fluxo de teste\n",
    "data_to_recognition = split_data_recognition(dataS2, column='subject', n_samples=50)\n",
    "\n",
    "_, _, external_users_data = split_data(external_users, column='subject', n_samples=50)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#  \n",
    "\n",
    "\n",
    " #Sistema com Adaptação (GraphMinCut)\n",
    "\n",
    "detector = M2005()\n",
    "adaptive = \"SlidingWindow\"\n",
    "system = BiometricSystem(detector=detector, random_state=R)\n",
    "system.enrollment_grafos(dataset=data_to_enrollment, adaptive=adaptive)\n",
    "\n",
    "decision_threshold = thresholds.best_threshold(data_to_validation, system, size=10, random_state=R)\n",
    "\n",
    "metrics_adaptativo_grafo_sliding = dict()\n",
    "\n",
    "lista_nao_usadas_grafo_sliding = list()\n",
    "lista_usadas_grafo_sliding = list()\n",
    "\n",
    "auxiliar_euclidiano2 = {}\n",
    "\n",
    "for j, genuine in enumerate(system.users.keys()):\n",
    "\n",
    "\n",
    "    ipd.clear_output(wait=True)\n",
    "    print(f\"Rodando GraphMinCutSliding Euclidiano\")\n",
    "    print(f\"Testando usuário {j+1}/{len(system.users.keys())}\")\n",
    "\n",
    "    datastream = data_stream.Random(impostor_rate= impostor_rate,\n",
    "                                    rate_external_impostor=rate_external_impostor,\n",
    "                                    random_state=R)\n",
    "\n",
    "    test_stream, y_true, amostras_grafo_sliding_genuinas,amostras_grafo_sliding_impostoras = datastream.create(genuine,\n",
    "                                                                                                data_to_recognition,\n",
    "                                                                                                external_users_data)\n",
    "\n",
    "\n",
    "\n",
    "    y_pred, lista_nao_usadas_grafo_sliding2, lista_usadas_grafo_sliding2 = system.autenticate_grafos(genuine,\n",
    "                                                                                test_stream,\n",
    "                                                                                decision_threshold=decision_threshold,\n",
    "                                                                                adaptive_TESTE=adaptive,\n",
    "                                                                                return_scores=False)\n",
    "\n",
    "    lista_nao_usadas_grafo_sliding.append(lista_nao_usadas_grafo_sliding2)\n",
    "    lista_usadas_grafo_sliding.append(lista_usadas_grafo_sliding2)\n",
    "\n",
    "    fmr, fnmr, b_acc,  y_genuine , y_impostor = system.compute_metrics(y_true, y_pred)\n",
    "        \n",
    "    count_1_genuine = y_genuine.value_counts().get(1, 0)\n",
    "    count_0_impostor = y_impostor.value_counts().get(0, 0)\n",
    "\n",
    "    auxiliar_euclidiano2[genuine] = (count_1_genuine, count_0_impostor)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for met in ['fmr','fnmr','b_acc']:\n",
    "        metrics_adaptativo_grafo_sliding.setdefault(genuine, dict()).setdefault(met,[]).append(eval(met))\n",
    "    #json.dump(metrics_adaptativo_grafo_sliding, open(\"metricas_grafo_sliding.json\", \"w\"))\n",
    "\n",
    "\n",
    "usuarios = metrics_adaptativo_grafo_sliding.keys()\n",
    "result = pd.DataFrame(metrics_adaptativo_grafo_sliding.values())\n",
    "\n",
    "fmr_mean = []\n",
    "fnmr_mean = []\n",
    "b_acc_mean = []\n",
    "\n",
    "for i in result['fmr']:\n",
    "    fmr_mean.append(Average(i))\n",
    "\n",
    "for i in result['fnmr']:\n",
    "    fnmr_mean.append(Average(i))\n",
    "\n",
    "for i in result['b_acc']:\n",
    "    b_acc_mean.append(Average(i))\n",
    "\n",
    "metrics_adaptativo_grafo_sliding_mean = pd.DataFrame(list(zip(usuarios, fmr_mean, fnmr_mean,b_acc_mean)),\n",
    "            columns =['Usuarios','fmr_mean', 'fnmr_mean','b_acc_mean'])\n",
    "\n",
    "GRAPH_MIN_CUT_SLIDING.append(metrics_adaptativo_grafo_sliding_mean['b_acc_mean'].mean())\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#     \n",
    "\n",
    "fim = timeit.default_timer()\n",
    "horas, rem = divmod(fim-inicio, 3600)\n",
    "minutos, segundos = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(horas),int(minutos),segundos))\n",
    "\n",
    "print('---------------------')\n",
    "print(\"Teste com Grafos distancia Euclidiana\")\n",
    "print(\"GRAPH_MIN_CUT_SLIDING\", GRAPH_MIN_CUT_SLIDING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
